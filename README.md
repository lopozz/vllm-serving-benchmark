# vllm-serving-benchmark
Follow this instructions to run inference benchmark using vllm backend
